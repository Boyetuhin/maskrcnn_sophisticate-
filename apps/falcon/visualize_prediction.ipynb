{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpha/virtualmachines/virtualenvs/py_3-6-5_2018-11-21/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import matplotlib.image as mpimg\n",
    "import skimage\n",
    "import glob\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "## Custom imports \n",
    "import utils.common as common\n",
    "from barchart import draw_and_save_line_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_evaluate_data(stats, sort_on, f1, f2):\n",
    "    iArr = {}\n",
    "    assort = { \"classname\":[] }\n",
    "    assort[sort_on] = []\n",
    "    assort[f1] = []\n",
    "    assort[f2] = []\n",
    "\n",
    "    \n",
    "    for i in range(len(stats[sort_on])):\n",
    "      iArr[stats[sort_on][i]] = { \"classname\": stats[\"classname\"][i] }\n",
    "      iArr[stats[sort_on][i]][sort_on] = stats[sort_on][i]\n",
    "      iArr[stats[sort_on][i]][f1] = stats[f1][i]\n",
    "      iArr[stats[sort_on][i]][f2] = stats[f2][i]\n",
    "  \n",
    "    \n",
    "    sorted_val = sorted(iArr,reverse=True)\n",
    "    for i in range(len(sorted_val)):\n",
    "      assort[\"classname\"].append(iArr[sorted_val[i]][\"classname\"])\n",
    "      assort[sort_on].append(iArr[sorted_val[i]][sort_on])\n",
    "      assort[f1].append(iArr[sorted_val[i]][f1])\n",
    "      assort[f2].append(iArr[sorted_val[i]][f2])\n",
    "\n",
    "    return assort\n",
    "\n",
    "\n",
    "def load_evaluate_data(evaluate_data_filepath):\n",
    "    if os.path.isfile(evaluate_data_filepath):\n",
    "        with open(evaluate_data_filepath) as json_file:  \n",
    "            evaluate_json_data = json.load(json_file)\n",
    "    else:\n",
    "        print(\"File Does Not Exists! evaluate_data_filepath: {}\".format(evaluate_data_filepath))\n",
    "    \n",
    "    return evaluate_json_data\n",
    "\n",
    "\n",
    "def get_stats(evaluate_data, username='', iou=0, evaluate_dir='', on=''):\n",
    "    \"\"\"Extracting desired values from original json in stats ignoring BG\"\"\"\n",
    "    evaluate_report = {\n",
    "        'username':username\n",
    "        ,'iou':iou\n",
    "        ,'evaluate_dir':evaluate_dir\n",
    "        ,'on':on\n",
    "        ,'classname':[]\n",
    "        ,'gt_total_annotation':[]\n",
    "        ,'pred_match_total_annotation':[]\n",
    "        ,'avg_precision':[]\n",
    "        ,'avg_recall':[]\n",
    "        ,'mAP':[]\n",
    "    }\n",
    "\n",
    "    for classname in evaluate_data.keys():\n",
    "        print(\"classname: {}\".format(classname))\n",
    "        if classname=='BG':\n",
    "            continue\n",
    "        evaluate_report['classname'].append(classname)\n",
    "        evaluate_report['gt_total_annotation'].append((evaluate_data[classname]['gt_total_annotation']))\n",
    "        evaluate_report['pred_match_total_annotation'].append(evaluate_data[classname]['pred_match_total_annotation'])\n",
    "        evaluate_report['mAP'].append(evaluate_data[classname]['mAP'])\n",
    "        \n",
    "        evaluate_report['avg_precision'].append(np.mean(evaluate_data[classname]['precisions']))\n",
    "        evaluate_report['avg_recall'].append(np.mean(evaluate_data[classname]['recalls']))\n",
    "        \n",
    "    def clean_cols(col):        \n",
    "        return ';'.join(str(e) for e in evaluate_report[col])\n",
    "    \n",
    "        \n",
    "    \n",
    "#     for k in evaluate_report.keys():\n",
    "#         evaluate_report[k] = clean_cols(k)\n",
    "    evaluate_report['classname'] = clean_cols('classname')\n",
    "    evaluate_report['gt_total_annotation'] = clean_cols('gt_total_annotation')\n",
    "    evaluate_report['pred_match_total_annotation'] = clean_cols('pred_match_total_annotation')\n",
    "    evaluate_report['mAP'] = clean_cols('mAP')\n",
    "    evaluate_report['avg_precision'] = clean_cols('avg_precision')\n",
    "    evaluate_report['avg_recall'] = clean_cols('avg_recall')\n",
    "    \n",
    "  \n",
    "    return evaluate_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_dir: 130519_185650-evaluate_050-hmd-train-270519_150700\n",
      "evaluate_plots_path: /mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_050-hmd-train-270519_150700/plots\n",
      "classification_rpt_files: ['/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_090-hmd-train-220519_161249/classification_rpt-train.json', '/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_090-hmd-train-220519_180553/classification_rpt-train.json', '/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_090-hmd-train-220519_164124/classification_rpt-train.json', '/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_090-hmd-train-220519_171004/classification_rpt-train.json', '/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_090-hmd-train-220519_173744/classification_rpt-train.json']\n"
     ]
    }
   ],
   "source": [
    "save_plots = True\n",
    "\n",
    "\n",
    "\n",
    "def clean_iou(iou):\n",
    "    return str(\"{:f}\".format(iou)).replace('.','')[:3]\n",
    "\n",
    "\n",
    "# json_file_path = '/mnt/vtq-69/logs/mask_rcnn/100519_112427-evaluate_050-hmd-train-160519_183815/'\n",
    "# json_file_name = 'classification_rpt-train.json'\n",
    "\n",
    "# evaluate_plots_dir = \"evaluation_hmd_20190329T001052\"\n",
    "# plots_path = \"/home/nikhil/Documents/ai-ml-dl-data/logs/mask_rcnn\"\n",
    "\n",
    "username = \"alpha\"\n",
    "# sysip = '69'\n",
    "base_path = \"/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn\"\n",
    "base_path = \"/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup\"\n",
    "\n",
    "##---\n",
    "iou = 0.90\n",
    "dataset_id = '130519_185650'\n",
    "exec_on_timestamp='220519_163838'\n",
    "subset = 'test'\n",
    "\n",
    "## 100519_112427-evaluate_050-hmd-train-270519_135830\n",
    "\n",
    "##---\n",
    "iou = 0.50\n",
    "dataset_id = '140519_193751'\n",
    "exec_on_timestamp='270519_150700'\n",
    "subset = 'train'\n",
    "\n",
    "iou = 0.50\n",
    "dataset_id = '130519_185650'\n",
    "exec_on_timestamp='270519_150700'\n",
    "subset = 'train'\n",
    "\n",
    "##---\n",
    "dbname='hmd'\n",
    "\n",
    "##---\n",
    "evaluate_dir = dataset_id+'-evaluate_'+clean_iou(iou)+'-'+dbname+'-'+subset+'-'+exec_on_timestamp\n",
    "print(\"evaluate_dir: {}\".format(evaluate_dir))\n",
    "\n",
    "evaluate_data_log_path = os.path.join(base_path, evaluate_dir)\n",
    "json_file='classification_rpt-'+subset+'.json'\n",
    "\n",
    "evaluate_reports_path = os.path.join(base_path, 'evaluate_reports')\n",
    "\n",
    "\n",
    "evaluate_plots_path = os.path.join(base_path, evaluate_dir, 'plots')\n",
    "print(\"evaluate_plots_path: {}\".format(evaluate_plots_path))\n",
    "\n",
    "evaluate_dir = dataset_id+'-evaluate_'+'*-'+dbname+'-'+subset+'-*'\n",
    "\n",
    "classification_rpt_files = glob.glob(os.path.join(base_path, evaluate_dir, json_file))\n",
    "print(\"classification_rpt_files: {}\".format(classification_rpt_files))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create evaluate_reports_path\n",
    "common.mkdir_p(evaluate_reports_path)\n",
    "if save_plots:\n",
    "    # print(evaluate_plots_path)\n",
    "    common.mkdir_p(evaluate_plots_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/evaluate_reports/evalaute_report-270519_180046.csv\n",
      "/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_090-hmd-train-220519_161249/classification_rpt-train.json\n",
      "evaluate_dir_ref: 130519_185650-evaluate_090-hmd-train-220519_161249\n",
      "classname: BG\n",
      "classname: lane_marking\n",
      "evaluate_stats: {'username': 'alpha', 'iou': '090', 'evaluate_dir': '130519_185650-evaluate_090-hmd-train-220519_161249', 'on': 'train', 'classname': 'lane_marking', 'gt_total_annotation': '4240', 'pred_match_total_annotation': '0', 'avg_precision': 'nan', 'avg_recall': 'nan', 'mAP': '0.0'}\n",
      "/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_090-hmd-train-220519_180553/classification_rpt-train.json\n",
      "evaluate_dir_ref: 130519_185650-evaluate_090-hmd-train-220519_180553\n",
      "classname: BG\n",
      "classname: lane_marking\n",
      "evaluate_stats: {'username': 'alpha', 'iou': '090', 'evaluate_dir': '130519_185650-evaluate_090-hmd-train-220519_180553', 'on': 'train', 'classname': 'lane_marking', 'gt_total_annotation': '4240', 'pred_match_total_annotation': '0', 'avg_precision': 'nan', 'avg_recall': 'nan', 'mAP': '0.0'}\n",
      "/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_090-hmd-train-220519_164124/classification_rpt-train.json\n",
      "evaluate_dir_ref: 130519_185650-evaluate_090-hmd-train-220519_164124\n",
      "classname: BG\n",
      "classname: lane_marking\n",
      "evaluate_stats: {'username': 'alpha', 'iou': '090', 'evaluate_dir': '130519_185650-evaluate_090-hmd-train-220519_164124', 'on': 'train', 'classname': 'lane_marking', 'gt_total_annotation': '4240', 'pred_match_total_annotation': '0', 'avg_precision': 'nan', 'avg_recall': 'nan', 'mAP': '0.0'}\n",
      "/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_090-hmd-train-220519_171004/classification_rpt-train.json\n",
      "evaluate_dir_ref: 130519_185650-evaluate_090-hmd-train-220519_171004\n",
      "classname: BG\n",
      "classname: lane_marking\n",
      "evaluate_stats: {'username': 'alpha', 'iou': '090', 'evaluate_dir': '130519_185650-evaluate_090-hmd-train-220519_171004', 'on': 'train', 'classname': 'lane_marking', 'gt_total_annotation': '4240', 'pred_match_total_annotation': '0', 'avg_precision': 'nan', 'avg_recall': 'nan', 'mAP': '0.0'}\n",
      "/mnt/remotemnt/vtd-69/aimldl-logs/mask_rcnn/bkup/130519_185650-evaluate_090-hmd-train-220519_173744/classification_rpt-train.json\n",
      "evaluate_dir_ref: 130519_185650-evaluate_090-hmd-train-220519_173744\n",
      "classname: BG\n",
      "classname: lane_marking\n",
      "evaluate_stats: {'username': 'alpha', 'iou': '090', 'evaluate_dir': '130519_185650-evaluate_090-hmd-train-220519_173744', 'on': 'train', 'classname': 'lane_marking', 'gt_total_annotation': '4240', 'pred_match_total_annotation': '0', 'avg_precision': 'nan', 'avg_recall': 'nan', 'mAP': '0.0'}\n"
     ]
    }
   ],
   "source": [
    "timestamp = (\"{:%d%m%y_%H%M%S}\").format(datetime.datetime.now())\n",
    "evaluate_report_filepath = os.path.join(evaluate_reports_path,'evalaute_report-'+timestamp+'.csv')\n",
    "print(evaluate_report_filepath)\n",
    "\n",
    "evaluate_stats_summary=[]\n",
    "for evaluate_data_filepath in classification_rpt_files:\n",
    "    # evaluate_data_filepath=os.path.join(evaluate_data_log_path, json_file)\n",
    "    evaluate_json_data=load_evaluate_data(evaluate_data_filepath)\n",
    "    # print(evaluate_json_data)\n",
    "    print(evaluate_data_filepath)\n",
    "    \n",
    "    evaluate_dir_ref = evaluate_data_filepath.split(os.path.sep)[-2]\n",
    "    print(\"evaluate_dir_ref: {}\".format(evaluate_dir_ref))\n",
    "    \n",
    "    iou_ref = evaluate_dir_ref.split('-')[1].split('_')[1]\n",
    "    evaluate_stats = get_stats(evaluate_json_data, username, iou_ref, evaluate_dir=evaluate_dir_ref, on=subset)\n",
    "    print(\"evaluate_stats: {}\".format(evaluate_stats))\n",
    "    evaluate_stats_summary.append(evaluate_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Dump to a csv file\n",
    "df = pd.DataFrame.from_dict(evaluate_stats_summary)\n",
    "# df.fillna(0)\n",
    "df = df.replace('nan', 0)\n",
    "df.to_csv(evaluate_report_filepath, index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   avg_precision  avg_recall     classname  \\\n",
      "0              0           0  lane_marking   \n",
      "1              0           0  lane_marking   \n",
      "2              0           0  lane_marking   \n",
      "3              0           0  lane_marking   \n",
      "4              0           0  lane_marking   \n",
      "\n",
      "                                        evaluate_dir gt_total_annotation  iou  \\\n",
      "0  130519_185650-evaluate_090-hmd-train-220519_16...                4240  090   \n",
      "1  130519_185650-evaluate_090-hmd-train-220519_18...                4240  090   \n",
      "2  130519_185650-evaluate_090-hmd-train-220519_16...                4240  090   \n",
      "3  130519_185650-evaluate_090-hmd-train-220519_17...                4240  090   \n",
      "4  130519_185650-evaluate_090-hmd-train-220519_17...                4240  090   \n",
      "\n",
      "   mAP     on pred_match_total_annotation username  \n",
      "0  0.0  train                           0    alpha  \n",
      "1  0.0  train                           0    alpha  \n",
      "2  0.0  train                           0    alpha  \n",
      "3  0.0  train                           0    alpha  \n",
      "4  0.0  train                           0    alpha  \n"
     ]
    }
   ],
   "source": [
    "# print(evaluate_stats_summary)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stats in evaluate_stats_summary:\n",
    "#     print(\"stats['gt_total_annotation'] : {}\".format(stats['gt_total_annotation'] ))\n",
    "#     print(\"stats: {}\".format(stats))\n",
    "#     print(\"gt_total_annotation: {}\".format(type(gt_total_annotation)))\n",
    "    \n",
    "#     for e in stats['gt_total_annotation']:\n",
    "#         print(e)\n",
    "    \n",
    "            \n",
    "# #     def str_to_int(col):  \n",
    "# #         for e in stats[col]:\n",
    "# #             return int(e)\n",
    "    \n",
    "# #     stats['gt_total_annotation'] = str_to_int('gt_total_annotation')\n",
    "# #     stats['pred_match_total_annotation'] = str_to_int('pred_match_total_annotation')\n",
    "    \n",
    "# #     stats['gt_total_annotation']=int(stats['gt_total_annotation'])\n",
    "# #     stats['pred_match_total_annotation']=int(stats['pred_match_total_annotation'])\n",
    "# #     stats['mAP']=float(stats['mAP'])\n",
    "   \n",
    "    \n",
    "    # pred_match_total_annotation sorting\n",
    "    iArr = {}\n",
    "    for i in range(len(stats[\"gt_total_annotation\"])):\n",
    "      iArr[stats[\"gt_total_annotation\"][i]] = {\"classname\":stats[\"classname\"][i],\"gt_total_annotation\":stats[\"gt_total_annotation\"][i],\"pred_match_total_annotation\":stats[\"pred_match_total_annotation\"][i], \"mAP\":stats[\"mAP\"][i]}\n",
    "    sortedVal = sorted(iArr,reverse=True)\n",
    "\n",
    "    sorted_ann = {\"classname\":[],\"gt_total_annotation\":[],\"pred_match_total_annotation\":[],\"mAP\":[]}\n",
    "    for i in range(len(sortedVal)):\n",
    "      sorted_ann[\"classname\"].append(iArr[sortedVal[i]][\"classname\"])\n",
    "      sorted_ann[\"gt_total_annotation\"].append(iArr[sortedVal[i]][\"gt_total_annotation\"])\n",
    "      sorted_ann[\"pred_match_total_annotation\"].append(iArr[sortedVal[i]][\"pred_match_total_annotation\"])\n",
    "      sorted_ann[\"mAP\"].append(iArr[sortedVal[i]][\"mAP\"])\n",
    "\n",
    "\n",
    "    print(sorted_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(col):\n",
    "    return int(str(e) for e in evaluate_stats_summary[col])\n",
    "def str_to_float(col):\n",
    "    return float(str(e) for e in evaluate_stats_summary[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in evaluate_stats_summary: \n",
    "    i['gt_total_annotation']=int(i['gt_total_annotation'])\n",
    "    i['pred_match_total_annotation']=int(i['pred_match_total_annotation'])\n",
    "    i['mAP']=float(i['mAP'])\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in evaluate_stats_summary: \n",
    "    print(type(i['mAP']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_evaluate_data(stats, sort_on, f1, f2):\n",
    " \n",
    "    \n",
    "    \n",
    "    iArr = {}\n",
    "    assort = { \"classname\":[] }\n",
    "    assort[sort_on] = []\n",
    "    assort[f1] = []\n",
    "    assort[f2] = []\n",
    "\n",
    "    \n",
    "    for i in range(len(stats[sort_on])):\n",
    "      iArr[stats[sort_on][i]] = { \"classname\": stats[\"classname\"][i] }\n",
    "      iArr[stats[sort_on][i]][sort_on] = stats[sort_on][i]\n",
    "      iArr[stats[sort_on][i]][f1] = stats[f1][i]\n",
    "      iArr[stats[sort_on][i]][f2] = stats[f2][i]\n",
    "  \n",
    "    \n",
    "    sorted_val = sorted(iArr,reverse=True)\n",
    "    for i in range(len(sorted_val)):\n",
    "      assort[\"classname\"].append(iArr[sorted_val[i]][\"classname\"])\n",
    "      assort[sort_on].append(iArr[sorted_val[i]][sort_on])\n",
    "      assort[f1].append(iArr[sorted_val[i]][f1])\n",
    "      assort[f2].append(iArr[sorted_val[i]][f2])\n",
    "\n",
    "    return assort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stats in evaluate_stats:\n",
    "    print(\"stats: {}\".format(stats))\n",
    "    \n",
    "    ##TODO: sort the sort!\n",
    "    ## gt_total_annotation sorting\n",
    "    sort_on=\"gt_total_annotation\"\n",
    "    f1=\"pred_match_total_annotation\"\n",
    "    f2=\"mAP\"\n",
    "    sorted_gt_total_annotation = sort_evaluate_data(stats, sort_on, f1, f2)\n",
    "    print(\"sorted_gt_total_annotation: {}\".format(sorted_gt_total_annotation))\n",
    "\n",
    "#     ## pred_match_total_annotation sorting\n",
    "#     f1=\"gt_total_annotation\"\n",
    "#     sort_on=\"pred_match_total_annotation\"\n",
    "#     f2=\"mAP\"\n",
    "\n",
    "#     sorted_pred_match_total_annotation = sort_evaluate_data(stats, sort_on, f1, f2)\n",
    "#     print(\"sorted_pred_match_total_annotation: {}\".format(sorted_pred_match_total_annotation))\n",
    "\n",
    "#     ## mAP sorting\n",
    "#     f1=\"gt_total_annotation\"\n",
    "#     f2=\"pred_match_total_annotation\"\n",
    "#     sort_on=\"mAP\"\n",
    "#     sorted_mAP = sort_evaluate_data(stats, sort_on, f1, f2)\n",
    "#     print(\"sorted_mAP: {}\".format(sorted_mAP))\n",
    "\n",
    "#     sorted_pred_match_total_annotation = stats['pred_match_total_annotation']\n",
    "#     sorted_gt_total_annotation = stats['gt_total_annotation']\n",
    "#     sorted_mAP = stats['mAP']\n",
    "    \n",
    "    ## plot_for gt_total_annotation\n",
    "    legend_label='Total Ground Truth Annotation'\n",
    "    plot_title='Total Annotations per Category over HMD Training Dataset'\n",
    "    scale=20000\n",
    "    offset=700\n",
    "    draw_and_save_line_graph(sorted_gt_total_annotation,sorted_gt_total_annotation['gt_total_annotation'],legend_label,plot_title,scale,offset)\n",
    "\n",
    "#     ## plot_for pred_match_total_annotation\n",
    "#     legend_label='Total Predicted Annotation'\n",
    "#     plot_title='Total Predicted Annotations per Category over HMD Training Dataset'\n",
    "#     scale=12000\n",
    "#     offset=400\n",
    "#     draw_and_save_line_graph(sorted_pred_match_total_annotation,sorted_pred_match_total_annotation['pred_match_total_annotation'],legend_label,plot_title,scale,offset)\n",
    "\n",
    "#     ## Read saved plot image and display here using matplotlib\n",
    "#     img=mpimg.imread(save_file)\n",
    "#     imgplot = plt.imshow(img)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "#     ## Read saved plot image and display here using skimage\n",
    "#     skimage.io.imshow(save_file)\n",
    "\n",
    "#     ## Rounding off the mAP values \n",
    "#     rounded_mAP = []\n",
    "#     for i in range(len(sorted_mAP['mAP'])):\n",
    "#         rounded_mAP.append(round(sorted_mAP['mAP'][i],2))\n",
    "#     print(rounded_mAP)\n",
    "\n",
    "#     ## plot_for mAP\n",
    "#     legend_label='mAP (mean Average Precision)'\n",
    "#     plot_title='Mean Average Precision per Category over HMD Training Dataset'\n",
    "#     scale=1\n",
    "#     offset=0.16\n",
    "#     draw_and_save_line_graph(sorted_mAP,rounded_mAP,legend_label,plot_title,scale,offset)\n",
    "\n",
    "#     gt_label='Total Ground Truth Annotation'\n",
    "#     pred_label='Total Predicted Annotation'\n",
    "#     mAP_label='mAP (Mean Avg. Precision)'\n",
    "#     plot_title='Total Annotations & mAP per Category over HMD Training Dataset'\n",
    "\n",
    "#     ##\n",
    "#     def plot_and_save_pred_stats(sorted_gt_total_annotation, gt_label, pred_label, mAP_label, plot_title):\n",
    "#         labels=sorted_gt_total_annotation['classname']\n",
    "#         x_pos_labels = np.arange(len(labels))\n",
    "#         gt_total_annotation=sorted_gt_total_annotation['gt_total_annotation']\n",
    "#         pred_match_total_annotation=sorted_gt_total_annotation['pred_match_total_annotation']\n",
    "#         mAP=sorted_gt_total_annotation['mAP']\n",
    "\n",
    "#         plt.plot(x_pos_labels, gt_total_annotation, 's-' , alpha=0.7 , label=gt_label )\n",
    "#         plt.plot(x_pos_labels, pred_match_total_annotation, 's-' , alpha=0.7 , label=pred_label,color='red')\n",
    "#         plt.plot(x_pos_labels, mAP, 's-' , alpha=0.7 , label=mAP_label, color='green')\n",
    "\n",
    "#         # plt.ylim(0,22000)\n",
    "#         plt.xticks(x_pos_labels, labels, rotation=80, fontsize=10)\n",
    "#         plt.title(plot_title)\n",
    "#         plt.rcParams[\"figure.figsize\"] = [22,15]\n",
    "#         plt.legend(loc='upper right', prop={'size': 18})\n",
    "#         # for a,b in zip(x_pos_labels, gt_total_annotation):\n",
    "#         #     c=b+1250\n",
    "#         #     plt.text(a, c, str(b),rotation='vertical',ha='center', fontsize=12)\n",
    "#         # for a,b in zip(x_pos_labels, pred_match_total_annotation):\n",
    "#         #     c=b+1250\n",
    "#         #     plt.text(a, c, str(b),rotation='vertical',ha='center', fontsize=12)\n",
    "#         # for a,b in zip(x_pos_labels, mAP):\n",
    "#         #     c=b+1250\n",
    "#         #     plt.text(a, c, str(b),rotation='vertical',ha='center', fontsize=12)\n",
    "#         plot_file_name=\"Total Annotations & mAP per Category over HMD Training Dataset\"\n",
    "#     #     save_file = os.path.join(evaluate_plots_path, plot_file_name+\".png\")\n",
    "#     #     plt.savefig(save_file, bbox_inches='tight', pad_inches=0.5 )\n",
    "\n",
    "#     plot_and_save_pred_stats(sorted_gt_total_annotation, gt_label, pred_label, mAP_label, plot_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stats in evaluate_stats_summary:\n",
    "\n",
    "    labels=stats['classname']\n",
    "    gt_total_annotation=stats['gt_total_annotation']\n",
    "    x_pos_labels = np.arange(len(labels))\n",
    "    plt.plot(x_pos_labels, gt_total_annotation, 's-' ,label='Total Ground Truth Annotation', alpha=0.7)\n",
    "    plt.ylim(0,400)\n",
    "    plt.xticks(x_pos_labels, labels, rotation=80, fontsize=10)\n",
    "    plt.title('Total Ground Truth Annotation per Category over HMD Training Dataset')\n",
    "    plt.legend(loc='upper right', prop={'size': 18})\n",
    "    plt.rcParams[\"figure.figsize\"] = [22,15]  \n",
    "    for a,b in zip(x_pos_labels, gt_total_annotation):\n",
    "        c=b+10\n",
    "        plt.text(a, c, str(b),rotation='vertical',ha='center')\n",
    "#     save_file = os.path.join(filepath,\"Total Ground Truth Annotation per Category over HMD Training Dataset\"+\".png\")\n",
    "#     plt.savefig(save_file, bbox_inches='tight' ,pad_inches=0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stats in evaluate_stats_summary:\n",
    "    labels=stats['classname']\n",
    "    avg_precision=stats['avg_precision']\n",
    "    avg_recall=stats['avg_recall']\n",
    "    mAP=stats['mAP']\n",
    "    x_pos_labels = np.arange(len(labels))\n",
    "    plt.plot(x_pos_labels, avg_precision, 's-' , alpha=0.7 , label='avg_precision' )\n",
    "    plt.plot(x_pos_labels, avg_recall, 's-' , alpha=0.7 , label='avg_recall',color='red')\n",
    "    plt.plot(x_pos_labels, mAP, 's-' , alpha=0.7 , label='mAP (Mean Avg. Precision)', color='green')\n",
    "\n",
    "    plt.ylim(0,10)\n",
    "    plt.xticks(x_pos_labels, labels, rotation=80, fontsize=18)\n",
    "    plt.title('Total Annotations & mAP per Category over HMD Training Dataset')\n",
    "    plt.rcParams[\"figure.figsize\"] = [22,15]\n",
    "    plt.legend(loc='upper right', prop={'size': 18})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x=[[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]]\n",
    "y=[[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]]\n",
    "for i in range(len(x)):\n",
    "    plt.figure()\n",
    "    plt.plot(x[i],y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
